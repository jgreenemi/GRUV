{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRUV - A Modified Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name implies, this will be my work on turning the GRUV repo from Stanford into a usable music-making utility, for music producers' benefit. Contributions are welcome, as this is made to turn a research project into a useful tool in a music maker's toolkit. Intermixed below will be the `README.md` contents for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRUV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRUV is a Python project for algorithmic music generation using recurrent neural networks.\n",
    "\n",
    "Note: This code works with Keras v. 0.1.0, later versions of Keras may not work.\n",
    "\n",
    "For a demonstration of our project on raw audio waveforms (as opposed to the standard MIDI), see here: https://www.youtube.com/watch?v=0VTI1BBLydE\n",
    "\n",
    "Copyright (C) 2015 Matt Vitelli matthew.vitelli@gmail.com and Aran Nayebi aran.nayebi@gmail.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dependencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use GRUV, you will first need to install the following dependencies:\n",
    "\n",
    "Theano: http://deeplearning.net/software/theano/#download\n",
    "\n",
    "Keras: https://github.com/fchollet/keras.git\n",
    "\n",
    "NumPy: http://www.numpy.org/\n",
    "\n",
    "SciPy: http://www.scipy.org/\n",
    "\n",
    "LAME (for MP3 source files): http://lame.sourceforge.net/ \n",
    "\n",
    "SoX (for FLAC source files): http://sox.sourceforge.net/\n",
    "\n",
    "Once that's taken care of, you can try training a model of your own as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your music into ./datasets/YourMusicLibrary/ and type the following command into Terminal:\n",
    ">    python convert_directory.py\n",
    "\n",
    "This will convert all mp3s in ./datasets/YourMusicLibrary/ into WAVs and convert the WAVs into a useful representation for the deep learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (parse_files.py, line 113)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"data_utils/parse_files.py\"\u001b[0;36m, line \u001b[0;32m113\u001b[0m\n\u001b[0;31m    def convert_wav_files_to_nptensor(directory, block_size: int, max_seq_len, out_file, max_files=20, useTimeDomain=False):\u001b[0m\n\u001b[0m                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Imports from convert_dictionary.py\n",
    "from data_utils.parse_files import *\n",
    "from pprint import pprint\n",
    "import config.nn_config as nn_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dictionary():\n",
    "    config = nn_config.get_neural_net_configuration()\n",
    "    input_directory = config['dataset_directory']\n",
    "    output_filename = config['model_file'] \n",
    "\n",
    "    freq = config['sampling_frequency'] #sample frequency in Hz\n",
    "    clip_len = 10  #length of clips for training. Defined in seconds\n",
    "    block_size = freq // 4  #block sizes used for training - this defines the size of our input state\n",
    "    max_seq_len = int(round((freq * clip_len) / block_size)) #Used later for zero-padding song sequences\n",
    "    #Step 1 - convert MP3s to WAVs\n",
    "    new_directory = convert_folder_to_wav(input_directory, freq)\n",
    "    #Step 2 - convert WAVs to frequency domain with mean 0 and standard deviation of 1\n",
    "    convert_wav_files_to_nptensor(new_directory, block_size, max_seq_len, output_filename)\n",
    "    print('Files converted!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_shape: (0, 40, 22050)\n",
      "Flushing to disk...\n",
      "Done!\n",
      "Files converted!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ubuntu/src/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:73: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    }
   ],
   "source": [
    "convert_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 2. Train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "At this point, you should have four files named YourMusicLibraryNP_x.npy, YourMusicLibraryNP_y.npy, YourMusicLibraryNP_var.npy, and YourMusicLibraryNP_mean.npy.\n",
    "\n",
    "YourMusicLibraryNP_x contains the input sequences for training\n",
    "YourMusicLibraryNP_y contains the output sequences for training\n",
    "YourMusicLibraryNP_mean contains the mean for each feature computed from the training set\n",
    "YourMusicLibraryNP_var contains the variance for each feature computed from the training set\n",
    "\n",
    "You can train your very first model by typing the following command into Terminal:\n",
    ">    python train.py\n",
    "\n",
    "Training will take a while depending on the length and number of songs used\n",
    "If you get an error of the following form:\n",
    "Error allocating X bytes of device memory (out of memory). Driver report Y bytes free and Z bytes total\n",
    "you must adjust the parameters in train.py - specifically, decrease the batch_size to something smaller. If you still have out of memory errors, you can also decrease the hidden_dims parameter in train.py and generate.py, although this will have a significant impact on the quality of the generated music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Notes\n",
    "\n",
    "I found that my Jupyter session was using `/usr/bin/python2.7` instead of the expected path at `/home/ubuntu/src/anaconda3/envs/gruv2/bin/python`.\n",
    "\n",
    "Some of the commands used below helped solve this issue:\n",
    "\n",
    "```bash\n",
    "  880  clear\n",
    "  881  tmux attach\n",
    "  882  sys.executable\n",
    "  885  conda install ipykernel\n",
    "  888  python -m ipykernel install --user\n",
    "  889  tmux attach\n",
    "  890  history\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/src/anaconda3/envs/gruv2/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3b917c037433>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnn_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnetwork_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_config\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/prjx/GRUV/nn_utils/network_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTimeDistributedDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_lstm_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_frequency_dimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hidden_dimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_recurrent_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import nn_utils.network_utils as network_utils\n",
    "import config.nn_config as nn_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    #TODO Config can be global for this notebook.\n",
    "    config = nn_config.get_neural_net_configuration()\n",
    "    inputFile = config['model_file']\n",
    "    cur_iter = 0\n",
    "    model_basename = config['model_basename']\n",
    "    model_filename = model_basename + str(cur_iter)\n",
    "\n",
    "    #Load up the training data\n",
    "    print ('Loading training data')\n",
    "    #X_train is a tensor of size (num_train_examples, num_timesteps, num_frequency_dims)\n",
    "    #y_train is a tensor of size (num_train_examples, num_timesteps, num_frequency_dims)\n",
    "    X_train = np.load(inputFile + '_x.npy')\n",
    "    y_train = np.load(inputFile + '_y.npy')\n",
    "    print ('Finished loading training data')\n",
    "\n",
    "    #Figure out how many frequencies we have in the data\n",
    "    freq_space_dims = X_train.shape[2]\n",
    "    hidden_dims = config['hidden_dimension_size']\n",
    "\n",
    "    #Creates a lstm network\n",
    "    model = network_utils.create_lstm_network(num_frequency_dimensions=freq_space_dims, num_hidden_dimensions=hidden_dims)\n",
    "    #You could also substitute this with a RNN or GRU\n",
    "    #model = network_utils.create_gru_network()\n",
    "\n",
    "    #Load existing weights if available\n",
    "    if os.path.isfile(model_filename):\n",
    "        model.load_weights(model_filename)\n",
    "\n",
    "    num_iters = 50             #Number of iterations for training\n",
    "    epochs_per_iter = 25    #Number of iterations before we save our model\n",
    "    batch_size = 5            #Number of training examples pushed to the GPU per batch.\n",
    "                            #Larger batch sizes require more memory, but training will be faster\n",
    "    print ('Starting training!')\n",
    "    while cur_iter < num_iters:\n",
    "        print('Iteration: ' + str(cur_iter))\n",
    "        #We set cross-validation to 0,\n",
    "        #as cross-validation will be on different datasets\n",
    "        #if we reload our model between runs\n",
    "        #The moral way to handle this is to manually split\n",
    "        #your data into two sets and run cross-validation after\n",
    "        #you've trained the model for some number of epochs\n",
    "        history = model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=epochs_per_iter, verbose=1, validation_split=0.0)\n",
    "        cur_iter += epochs_per_iter\n",
    "    print ('Training complete!')\n",
    "    model.save_weights(model_basename + str(cur_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've finished training your model, it's time to generate some music!\n",
    "Type the following command into Terminal:\n",
    ">    python generate.py\n",
    "\n",
    "After some amount of time, you should have a file called generated_song.wav\n",
    "\n",
    "Future work:\n",
    "Improve generation algorithms. Our current generation scheme uses the training / testing data as a seed sequence, which tends to produce verbatum copies of the original songs. One might imagine that we could improve these results by taking linear combinations of the hidden states for different songs and projecting the combinations back into the frequency space and using those as seed sequences. You can find the core components of the generation algorithms in gen_utils/seed_generator.py and gen_utils/sequence_generator.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
